---
title: "Repoer"
author: "Yihan Shi, Medy Mu, Edna Zhang"
date: "10/31/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(plyr)
library(gam)
library(boot)
library(splines)
library(kableExtra)
library(glmnet)
```

```{r}
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
set.seed(1)
```

```{r}
# transformation
asymptote <- 25
scale <- 2
midpoint <- 0.3
train$Fr_logit <-  asymptote / (1 + exp((midpoint - train$Fr) * scale))
train$Fr_logit
```

```{r}
#convert to central moments
train$C_moment_1 <- 0
train$C_moment_2 <- train$R_moment_2 - (train$R_moment_1)^2
train$C_moment_3 <- train$R_moment_3 - 3*train$R_moment_1*train$R_moment_2 + 2*(train$R_moment_1)^3
train$C_moment_4 <- train$R_moment_4 - 4*train$R_moment_1*train$R_moment_3 + 6*(train$R_moment_1)^2*train$R_moment_2 -3*(train$R_moment_1)^4
```

```{r}
train$St_log <- log(train$St)
train$R_moment_3_log <- log(train$R_moment_3)
train
```

# Introduction

"We are experiencing some turbulence, please fasten your seat belt." Many of us might have heard this radio on the plane and felt bumpy. When we mix paint in water, we can also observe turbulence as the color dissipates. Turbulence is so common and easily observed in daily life, yet its causes and effects are hard to predict. In fluid dynamics, turbulence is "characterized by chaotic changes in pressure and flow velocity". With some knowledge and observation in parameters such as fluid density, flow speed, and the property of particles that cluster inside turbulent flows, we can gain insights into the distribution and clustering of particles in idealized turbulence. In this case study, we will investigate 3 observed features that might contribute to particle distribution in turbulence: Reynolds number (Re), which takes flow speed, viscosity, and density into account; Gravitational acceleration (Fr); Stokes number (St) that quantifies particle characteristics like size, relaxation time, and particle diameter. We hope to use these 3 features to explain changes in particle distribution as well as extrapolate beyond the scope of the known observations.

# Model

## Raw Moment 1
```{r}
plot(density(train$R_moment_1))
plot(density(log(train$R_moment_1)))
```

Since R_moment_1 is skewed, we do a log transformation to make it more normal.

### Linear regression
```{r}
df.shuffled <- train[sample(nrow(train)),]
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)), breaks=K, labels=FALSE)

#create object to hold MSE's of models
mse <- rep(NA, K)
adj.r2 <- rep(NA, K)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train <-  lm(log(R_moment_1) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test <- predict(fit.train, newdata=testData)
    mse[i] <- mean((exp(fit.test-testData$R_moment_1))^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
}

#find MSE
mse_linear <- mean(mse)
mse_adjr2 <- mean(adj.r2)
```

### Polynomial regression
```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_1) ~ poly(St,j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((exp(fit.test-testData$R_moment_1))^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

x <- 1:10
mean_mse_linear <- colMeans(mse)
min_mse_linear <- min(mse)
mean_r2_linear <- colMeans(adj.r2)
max_r2_linear <- max(adj.r2)

plot(x, mean_mse_linear, type = "l", xlab = "Polynomial Degree", ylab = "MSE", main = "Polynomial regression")
plot(c(1:10), mean_r2_linear, xlab = "Polynomial Degree", ylab = "Adjusted R^2", main = "Polynomial regression", type = "l")
```

Using cross-validation with 5 folds, we see the adjusted $R^2$ decreases as polynomial degree increases. The MSE indicates that linear regression is no sufficient. The minimum MSE is achieved with polynomial order 2 on St. We decided that the optimal polynomial order for St is 2 for better interpretation.

We tried removing data of both high leverage and residual. However, this didn't change the MSE and adjusted $R^2$ greatly. Since these observations take up 7% of the full training data, we decided that we don't want to exclude them.

### Natural spline
```{r}
df <- 13

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=df)
adj.r2 <- matrix(data=NA,nrow=K,ncol=df)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:df){
        fit.train <- glm(log(R_moment_1) ~ ns(St, df = j) + Fr_logit + Re + Fr_logit * Re,
           data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        # adj.r2[i,j] <- summary(fit.train)$adj.r.squared
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((exp(fit.test-testData$R_moment_1))^2) 
    }
}

#find MSE for each degree 
mean_mse_ns <- colMeans(mse)
min_mse_ns <- min(mse)
mean_r2_ns <- colMeans(adj.r2)
max_r2_ns <- max(adj.r2)

plot(c(1:df), mean_mse_ns, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "Natural spline")
plot(c(1:df), mean_r2_ns, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "Natural spline")
```

Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases, and the lowest MSE is achieved at degree of 7. The optimal polynomial degrees of freedom for St is 7, which achieves the lowest MSE and a decent adjusted $R^2$.

### GAM
```{r}
df <- 20

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=df)
adj.r2 <- matrix(data=NA,nrow=K,ncol=df)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:df){
        fit.train <- gam(log(R_moment_1) ~ s(St, df = j) + Fr_logit + Re + Fr_logit * Re,
                              data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((exp(fit.test-testData$R_moment_1))^2) 
    }
}


#find MSE for each degree 
mean_mse_gam <- colMeans(mse)
min_mse_gam <- min(mse)
mean_r2_gam <- colMeans(adj.r2)
max_r2_gam <- max(adj.r2)

plot(c(1:df), mean_mse_gam, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "GAM")
plot(c(1:df), mean_r2_gam, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "GAM")
```

Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases to an extent (around 13). The optimal degrees of freedom for GAM model to achieve the lowest MSE is around 5, but MSE stops increasing after lifting degrees of freedom to 13. For more prediction power, we conclude that a degree of freedom around 13 is optimal.

```{r}
# make a MSE table
models <- c("Linear regresion", 
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_linear,
         min_mse_linear, 
         min_mse_ns, 
         min_mse_gam)
adj.R <- c(mse_adjr2,
           max_r2_linear,
           max_r2_ns,
           max_r2_gam)
formula <- c("R_moment_1 ~ Fr + Re + St + Fr * Re",
             "R_moment_1 ~ Fr + Re + poly(St, 2) + Fr * Re",
             "R_moment_1 ~ ns(St, df = 13) + Fr + Re + Fr * Re",
             "R_moment_1 ~ s(St, 13) + Re + Fr + Fr * Re"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kable(caption = "Model MSE and Adjusted $R^2$") %>% 
  kable_styling() %>% 
  add_footnote(c("Fr logit-transformed"), notation = "symbol")
```


Summary: For Raw_moment_1 (mean), GAM has the best results since it presents the highest adjusted $R^2$ and the lowest MSE. The second best model is natural spline, with only slight increase in MSE and decrease in adjusted $R^2$. Since both are hard to interpret, we use it for prediction and polynomial regression for inference.



## Raw Moment 2

```{r}
plot(density(train$R_moment_2))
plot(density(log(train$R_moment_2)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_2), type = "b")
plot(train$Re, log(train$R_moment_2), type = "b")
```

```{r}
drop <- c("St", "Fr_logit", "Re")
cor <- cor(train[,(names(train) %in% drop)])
cor
```

### Least Square Regression

```{r}
# least square model
r2_ls <- lm(log(R_moment_2) ~ St + Re + Fr_logit, data = train)
r2_ls_int <- lm(log(R_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(r2_ls)
summary(r2_ls_int)
```

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(R_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```

```{r}
par(mfrow=c(2,2))
plot(r2_ls)
```

```{r}
par(mfrow=c(2,2))
plot(r2_ls_int)
```

### Polynomial Model

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_2) ~ poly(St,j) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,5])
adjR2_poly
```

```{r}
r2_poly <- lm(log(R_moment_2) ~ poly(St,5) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = train)
summary(r2_poly)
```

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#define number of folds to use for k-fold cross-validation
K <- 10

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(R_moment_2) ~ ns(St, df = j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,6])
adjR2_spline

```

```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```

```{r}
r2_spline <- glm(log(R_moment_2) ~ ns(St, df = 6) + Re + Fr_logit + Re*Fr_logit, data = train)
attr(ns(sub_train$St, df = 6), "knots")
summary(r2_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r2_spline)
```

### Generalized Additive Model

```{r}
#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_2) ~ ns(St, j) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2), data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,4])
adjR2_gam
```

```{r}
r2_gam <- gam(log(R_moment_2) ~ ns(St, df = 4) + Re + ns(Fr_logit, df = 2) + Re:ns(Fr_logit,2), data = train)
```

```{r}
par(mfrow = c(1,3))
plot(r2_gam, se = TRUE, col = "blue")
```

```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(R_moment_2) ~ Fr + Re + St + Fr * Re",
            "log(R_moment_2) ~ poly(Fr,2) + Re + poly(St, 5) + Fr * Re",
             "log(R_moment_2) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(R_moment_2) ~ ns(St, 4) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```

## Raw Moment 3

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
plot(density(train$St))
```

```{r}
plot(density(log(train$St)))
```

```{r}
plot(density(train$R_moment_3))
```

```{r}
plot(density(log(train$R_moment_3)))
```


```{r}
var <- c("St_log", "Re", "Fr_logit", "R_moment_3_log")
cor <- pairs(train[,(names(train) %in% var)])
cor
```

## Least Square Regression

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5
#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
#create object to hold MSE's of models
linear_mse = rep(0,5)
linear_R2 <- rep(0,5)
#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    linear = lm(log(R_moment_3)~St_log+Re+Fr_logit, data = trainData)
    fit.test = predict(linear, newdata=testData)
    linear_mse[i] = mean((exp(fit.test)-testData$R_moment_3)^2) 
    linear_R2[i] <- summary(linear)$adj.r.squared
}
```

```{r}
summary(linear)
```

```{r}
plot(linear)
```

```{r}
mse_linear <- mean(linear_mse)
mse_linear
R2_linear <- max(linear_R2)
R2_linear
```

### Least Square Regression with Interactions

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5
#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
#create object to hold MSE's of models
interaction_mse = rep(0,5)
interaction_R2 <- rep(0,5)
#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    interaction = lm(log(R_moment_3)~St_log+Re+Fr_logit+Re*Fr_logit, data = trainData)
    fit.test = predict(interaction, newdata=testData)
    interaction_mse[i] = mean((exp(fit.test)-testData$R_moment_3)^2) 
    interaction_R2[i] <- summary(interaction)$adj.r.squared
}
```

```{r}
summary(interaction)
```

```{r}
plot(interaction)
```

```{r}
mse_interaction <- mean(interaction_mse)
mse_interaction
R2_interaction <- max(interaction_R2)
R2_interaction
```

### Polynomial Model

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5
#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
#create object to hold MSE's of models
poly_mse = matrix(data=NA,nrow=K,ncol=10)
poly_R2 <- matrix(data=NA,nrow=K,ncol=10)
#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:10){
        poly = lm(log(R_moment_3) ~ poly(St_log,j)+Re+poly(Fr_logit,2)+Re*Fr_logit, data = trainData)
        fit.test = predict(poly, newdata=testData)
        poly_mse[i,j] = mean((exp(fit.test)-testData$R_moment_3)^2) 
        poly_R2[i,j] <- summary(poly)$adj.r.squared
    }
}
```

```{r}
colMeans(poly_mse)
colMeans(poly_R2)
min(colMeans(poly_mse))
max(colMeans(poly_R2))
```

```{r}
poly <- lm(log(R_moment_3) ~ poly(St_log,3)+Re+poly(Fr_logit,2)+Re*poly(Fr_logit,2), data = train)
```

```{r}
summary(poly)
```

```{r}
plot(poly)
```

```{r}
mse_poly <- colMeans(poly_mse)[3]
mse_poly
R2_poly <- colMeans(poly_R2)[3]
R2_poly
```

```{r}
length(unique(train$St))
```

### Natural Spline

```{r}
RSS <- rep(0,13)
for (i in 3:13) {
  spline <- glm(log(R_moment_2) ~ bs(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
  RSS[i] <- sum(spline$residuals^2)
}
plot(3:13, RSS[3:13], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5
#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
#create object to hold MSE's of models
spline_mse = matrix(data=NA,nrow=K,ncol=13)
spline_R2 <- matrix(data=NA,nrow=K,ncol=13)
#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 3:13){
        spline = glm(log(R_moment_3) ~ ns(St_log, df = j)+Re+poly(Fr_logit,2)+Re*Fr_logit, data = trainData)
        fit.test = predict(spline, newdata=testData)
        spline_mse[i,j] = mean((exp(fit.test)-testData$R_moment_3)^2) 
        spline_R2[i,j] <- with(summary(spline), 1 - deviance/null.deviance)
    }
}
```

```{r}
colMeans(spline_mse)
colMeans(spline_R2)
min(colMeans(spline_mse), na.rm=TRUE)
max(colMeans(spline_R2), na.rm=TRUE)
```

```{r}
spline <- glm(log(R_moment_3) ~ ns(St_log, df = 3)+Re+poly(Fr_logit,2)+Re*Fr_logit, data = trainData)
summary(spline)
```

```{r}
plot(spline)
```

```{r}
mse_spline <- colMeans(spline_mse)[3]
mse_spline
R2_spline <- colMeans(spline_R2)[3]
R2_spline
```

```{r}
# make a MSE table
models <- c("Least square regression",
            "Interaction", 
            "Polynomial regression", 
            "Natural spline")
mse <- c(mse_linear, 
         mse_interaction, 
         mse_poly,
         mse_spline)
R2 <- c(R2_linear,
        R2_interaction, 
        R2_poly,
        R2_spline)
formula <- c("log(R_moment_3) ~ log(St)+Re+logit(Fr)",
            "log(R_moment_3) ~ log(St)+Re+logit(Fr)+Re*logit(Fr)",
             "log(R_moment_3) ~ poly(log(St),3)+Re+poly(logit(Fr),2)+Re*logit(Fr)",
             "log(R_moment_3) ~ ns(log(St),df=3)+Re+poly(logit(Fr),2)+Re*logit(Fr)"
             )
df <- data.frame(models, formula, mse, R2)
df %>% 
  kbl() %>% 
  kable_styling()
```


## Raw Moment 4

```{r}
plot(density(train$R_moment_4))
plot(density(log(train$R_moment_4)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_4), type = "b")
plot(train$Re, log(train$R_moment_4), type = "b")
```

### Least Square Regression

```{r}
# least square model
r4_ls <- lm(log(R_moment_4) ~ St + Re + Fr_logit, data = train)
r4_ls_int <- lm(log(R_moment_4) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(r4_ls)
summary(r4_ls_int)
```

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(R_moment_4) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$R_moment_4)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```

```{r}
#ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls_int)
```

### Polynomial Model

```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_4) ~ poly(St,j) + Re + poly(Fr_logit,2) + Re*Fr_logit , data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_4)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,2])
adjR2_poly
```


```{r}
r4_poly <- lm(log(R_moment_4) ~ poly(St,2) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = train)
summary(r4_poly)
```



### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_4) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#define number of folds to use for k-fold cross-validation
K <- 10

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(R_moment_4) ~ ns(St, df = j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,6])
adjR2_spline

```

```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
r4_spline <- glm(log(R_moment_4) ~ ns(St, df = 6) + Re + Fr_logit + Re*Fr_logit, data = train)
attr(ns(train$St, df = 6), "knots")
summary(r4_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r4_spline)
```

### Generalized Additive Model

```{r}
#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_4) ~ ns(St, j) + Re + ns(Fr_logit,2) + Re*ns(Fr_logit,2) , data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,1])
adjR2_gam
```


```{r}
r4_gam <- gam(log(R_moment_4) ~ St + Re + ns(Fr_logit,2) + Re*ns(Fr_logit,2) , data = train)
```

```{r}
par(mfrow = c(1,3))
plot(r4_gam, se = TRUE, col = "blue")
```

```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(R_moment_4) ~ Fr + Re + St + Fr * Re",
            "log(R_moment_4) ~ poly(Fr,2) + Re + poly(St, 2) + Fr * Re",
             "log(R_moment_4) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(R_moment_4) ~ St + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```

# C_moment_2

```{r}
plot(density(train$C_moment_2))
plot(density(log(train$C_moment_2)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$C_moment_2), type = "b")
plot(train$Re, log(train$C_moment_2), type = "b")
```

```{r}
drop <- c("St", "Fr_logit", "Re")
cor <- cor(train[,(names(train) %in% drop)])
cor
```

```{r}
# least square model
c2_ls <- lm(log(C_moment_2) ~ St + Re + Fr_logit, data = train)
c2_ls_int <- lm(log(C_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(c2_ls)
summary(c2_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(C_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$C_moment_2)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```


```{r}
#ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(c2_ls)
```
```{r}
par(mfrow=c(2,2))
plot(c2_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(C_moment_2) ~ poly(St,j) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_2)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```


```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,2])
adjR2_poly
```

```{r}
c2_poly <- lm(log(C_moment_2) ~ poly(St,2) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = train)
summary(r2_poly)
```

```{r}
#poly_pred <- predict(r2_poly, newdata=sub_test)
#mean((exp(poly_pred)-sub_test$R_moment_2)^2) 
```


## Non linear modeling

### Natural spline

```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(C_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(C_moment_2) ~ ns(St, df = j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,6])
adjR2_spline

```

```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
#set.seed(1)
#cv.err <- rep(0,15)
#for (i in 4:15) {
#model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
#cv.err[i] <- cv.glm(train, model.fit, K=5)$delta[1]
#}

#plot(4:15, cv.err[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
c2_spline <- glm(log(C_moment_2) ~ ns(St, df = 6) + Re + Fr_logit + Re*Fr_logit, data = train)
attr(ns(train$St, df = 6), "knots")
summary(c2_spline)
```

```{r}
par(mfrow = c(2,2))
plot(c2_spline)
```

```{r}
#spline_pred <- predict(r2_spline, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(spline_pred))^2)
```


```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 10

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(C_moment_2) ~ ns(St, j) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2), data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,4])
adjR2_gam
```

```{r}
c2_gam <- gam(log(R_moment_2) ~ ns(St, df = 4) + Re + ns(Fr_logit, df = 2) + Re:ns(Fr_logit,2), data = train)
```

```{r}
par(mfrow = c(1,3))
plot(c2_gam, se = TRUE, col = "blue")
```



```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(C_moment_2) ~ Fr + Re + St + Fr * Re",
            "log(C_moment_2) ~ poly(Fr,2) + Re + poly(St, 2) + Fr * Re",
             "log(C_moment_2) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(C_moment_2) ~ ns(St, 4) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```

## C_moment_4

```{r}
plot(density(train$C_moment_4))
plot(density(log(train$C_moment_4)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$C_moment_4), type = "b")
plot(train$Re, log(train$C_moment_4), type = "b")
```



```{r}
# least square model
c4_ls <- lm(log(C_moment_4) ~ St + Re + Fr_logit, data = train)
c4_ls_int <- lm(log(C_moment_4) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(c4_ls)
summary(c4_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(C_moment_4) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$C_moment_4)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```

```{r}
#ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(c4_ls)
```

```{r}
par(mfrow=c(2,2))
plot(c4_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(C_moment_4) ~ poly(St,j) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_4)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,2])
adjR2_poly
```


```{r}
c4_poly <- lm(log(R_moment_4) ~ poly(St,2) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = train)
summary(c4_poly)
```

## Non linear modeling

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(C_moment_4) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(C_moment_4) ~ ns(St, df = j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,4])
adjR2_spline

```


```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
c4_spline <- glm(log(C_moment_4) ~ ns(St, df = 6) + Re + Fr_logit + Re*Fr_logit, data = train)
attr(ns(train$St, df = 4), "knots")
summary(c4_spline)
```

```{r}
par(mfrow = c(2,2))
plot(c4_spline)
```

```{r}
#spline_pred <- predict(r2_spline, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(spline_pred))^2)
```


```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(C_moment_4) ~ ns(St, j) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2), data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$C_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,2])
adjR2_gam
```


```{r}
c4_gam <- gam(log(C_moment_2) ~ St + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2), data = train)
```

```{r}
par(mfrow = c(1,3))
plot(c4_gam, se = TRUE, col = "blue")
```


```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(C_moment_4) ~ Fr + Re + St + Fr * Re",
            "log(C_moment_4) ~ poly(Fr,2) + Re + poly(St, 2) + Fr * Re",
             "log(C_moment_4) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(C_moment_4) ~ St + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```
