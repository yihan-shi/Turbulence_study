---
title: "medy_case_study"
author: "Medy Mu"
date: '2022-10-31'
output: html_document
---

```{r}
library(dplyr)
library(glmnet)
library(splines)
library(boot)
library(gam)
library(gamreg)
library(kableExtra)
```

```{r}
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
```

```{r}
asymptote <- 25
scale <- 2
midpoint <- 0.3
train$Fr_logit <-  asymptote / (1 + exp((midpoint - train$Fr) * scale))
train$Fr_logit
```


```{r}
#convert to central moments
train$C_moment_1 <- 0
train$C_moment_2 <- train$R_moment_2 - (train$R_moment_1)^2
train$C_moment_3 <- train$R_moment_3 - 3*train$R_moment_1*train$R_moment_2 + 2*(train$R_moment_1)^3
train$C_moment_4 <- train$R_moment_4 - 4*train$R_moment_1*train$R_moment_3 + 6*(train$R_moment_1)^2*train$R_moment_2 -3*(train$R_moment_1)^4
```

```{r}
#set.seed(1)
#sub_train <- sample(1:nrow(train), nrow(train)*0.7) ## split data into train and test
#sub_test <- train[-sub_train,]
#sub_train <- train[sub_train,]
```

```{r}
plot(density(train$R_moment_2))
plot(density(log(train$R_moment_2)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_2), type = "b")
plot(train$Re, log(train$R_moment_2), type = "b")
```

```{r}
drop <- c("St", "Fr_logit", "Re")
cor <- cor(train[,(names(train) %in% drop)])
cor
```

```{r}
# least square model
r2_ls <- lm(log(R_moment_2) ~ St + Re + Fr_logit, data = train)
r2_ls_int <- lm(log(R_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(r2_ls)
summary(r2_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(R_moment_2) ~ St + Re + Fr_logit + Re*Fr_logit, data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```


```{r}
#ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(r2_ls)
```
```{r}
par(mfrow=c(2,2))
plot(r2_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_2) ~ poly(St,j) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
adj.r2
```


```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,5])
adjR2_poly
```

```{r}
r2_poly <- lm(log(R_moment_2) ~ poly(St,5) + Re + poly(Fr_logit,2) + Re*Fr_logit, data = train)
summary(r2_poly)
```
```{r}
#poly_pred <- predict(r2_poly, newdata=sub_test)
#mean((exp(poly_pred)-sub_test$R_moment_2)^2) 
```


## Non linear modeling

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(R_moment_2) ~ ns(St, df = j) + Re + Fr_logit + Re*Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,6])
adjR2_spline

```

```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
#set.seed(1)
#cv.err <- rep(0,15)
#for (i in 4:15) {
#model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
#cv.err[i] <- cv.glm(train, model.fit, K=5)$delta[1]
#}

#plot(4:15, cv.err[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
r2_spline <- glm(log(R_moment_2) ~ ns(St, df = 6) + Re + Fr_logit + Re*Fr_logit, data = train)
attr(ns(sub_train$St, df = 6), "knots")
summary(r2_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r2_spline)
```

```{r}
#spline_pred <- predict(r2_spline, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(spline_pred))^2)
```


```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_2) ~ ns(St, j) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2), data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,4])
adjR2_gam
```

```{r}
r2_gam <- gam(log(R_moment_2) ~ ns(St, df = 4) + Re + ns(Fr_logit, df = 2) + Re:ns(Fr_logit,2), data = train)
```

```{r}
par(mfrow = c(1,3))
plot(r2_gam, se = TRUE, col = "blue")
```



```{r}
#gam_pred <- predict(r2_gam, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(gam_pred))^2)
```

```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(R_moment_2) ~ Fr + Re + St + Fr * Re",
            "log(R_moment_2) ~ poly(Fr,2) + Re + poly(St, 5) + Fr * Re",
             "log(R_moment_2) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(R_moment_2) ~ ns(St, 4) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```

## R_moment_4

```{r}
plot(density(train$R_moment_4))
plot(density(log(train$R_moment_4)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_4), type = "b")
plot(train$Re, log(train$R_moment_4), type = "b")
```



```{r}
# least square model
r4_ls <- lm(log(R_moment_4) ~ St + Re + Fr_logit, data = train)
r4_ls_int <- lm(log(R_moment_4) ~ St + Re + Fr_logit + Re*Fr_logit, data = train)
summary(r4_ls)
summary(r4_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = rep(0,5)
adj.r2 <- rep(0,5)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train = lm(log(R_moment_4) ~ St + Re + Fr_logit , data = trainData)
    fit.test = predict(fit.train, newdata=testData)
    mse[i] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
    
}

#find MSE
mean(mse)

```

```{r}
mse_ls <- mean(mse)
mse_ls

adjR2_ls <- max(adj.r2)
adjR2_ls
```

```{r}
#ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls_int)
```

```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_4) ~ poly(St,j) + Re + Fr_logit, data = trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
adj.r2
```


```{r}
mse_poly<- min(colMeans(mse))
mse_poly

adjR2_poly <- max(adj.r2[,2])
adjR2_poly
```


```{r}
r4_poly <- lm(log(R_moment_4) ~ poly(St,2) + Re + Fr_logit, data = train)
summary(r4_poly)
```


```{r}
#poly_pred <- predict(r2_poly, newdata=sub_test)
#mean((exp(poly_pred)-sub_test$R_moment_2)^2) 
```


## Non linear modeling

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_4) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 4:degree){
        fit.train =  glm(log(R_moment_4) ~ ns(St, df = j) + Re + Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```
```{r}
mse_spline<- min(colMeans(mse)[4:15])
mse_spline

adjR2_spline <- max(adj.r2[,4])
adjR2_spline

```


```{r}
plot(4:15, colMeans(mse)[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```



```{r}
#set.seed(1)
#cv.err <- rep(0,15)
#for (i in 4:15) {
#model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re + Fr_logit + Re*Fr_logit, data = train)
#cv.err[i] <- cv.glm(train, model.fit, K=5)$delta[1]
#}

#plot(4:15, cv.err[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
r4_spline <- glm(log(R_moment_4) ~ ns(St, df = 4) + Re + Fr_logit, data = train)
attr(ns(train$St, df = 4), "knots")
summary(r4_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r4_spline)
```

```{r}
#spline_pred <- predict(r2_spline, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(spline_pred))^2)
```


```{r}
set.seed(1)
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_4) ~ ns(St, j) + Re + Fr_logit, data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
mse_gam<- min(colMeans(mse))
mse_gam

adjR2_gam <- max(adj.r2[,2])
adjR2_gam
```


```{r}
r4_gam <- gam(log(R_moment_2) ~ ns(St, df = 2) + Re + Fr_logit, data = train)
```

```{r}
par(mfrow = c(1,3))
plot(r4_gam, se = TRUE, col = "blue")
```




```{r}
#gam_pred <- predict(r2_gam, newdata = sub_test)
#mean((sub_test$R_moment_2 - exp(gam_pred))^2)
```

```{r}
# make a MSE table
models <- c("Least square regression",
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_ls, 
         mse_poly, 
         mse_spline,
         mse_gam)
adj.R <- c(adjR2_ls,
           adjR2_poly,
           adjR2_spline,
           adjR2_gam)
formula <- c("log(R_moment_4) ~ Fr + Re + St",
            "log(R_moment_4) ~ Fr + Re + poly(St, 5) + Fr * Re",
             "log(R_moment_4) ~ ns(St, df = 6) + Fr + Re + Fr * Re",
             "log(R_moment_4) ~ ns(St, 4) + Re + ns(Fr_logit,2) + Re:ns(Fr_logit,2)"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```


– How does each of the three parameters (Re, Fr, St) affect the distribution of particle cluster volumes? Do these effects appear linear or nonlinear? Try to interpret these effects using what the three parameters mean physically.
– Are the effects identified above similar over all central moments (i.e., over all response variables), or are there effects which differ between, say, the mean and the variance? Try to interpret the latter effects using the three parameters mean physically.
– Try to interpret any interaction effects using what the three parameters mean physically.

```{r}
summary(r4_poly)
```
Reynolds number Re, gravitational acceleration Fr, and particle characteristic St (details in presentation slides)
