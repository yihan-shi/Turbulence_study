---
title: "medy_case_study"
author: "Medy Mu"
date: '2022-10-31'
output: html_document
---

```{r}
library(glmnet)
library(splines)
library(boot)
library(gam)
library(gamreg)
```

```{r}
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
```

```{r}
train$Fr_factor <- factor(train$Fr)
test$Fr_factor <- factor(test$Fr)
train$Re_factor <- factor(train$Re)
test$Re_factor <- factor(test$Re)
```

```{r}
#convert to central moments
train$C_moment_1 <- 0
train$C_moment_2 <- train$R_moment_2 - (train$R_moment_1)^2
train$C_moment_3 <- train$R_moment_3 - 3*train$R_moment_1*train$R_moment_2 + 2*(train$R_moment_1)^3
train$C_moment_4 <- train$R_moment_4 - 4*train$R_moment_1*train$R_moment_3 + 6*(train$R_moment_1)^2*train$R_moment_2 -3*(train$R_moment_1)^4
```

```{r}
set.seed(1)
sub_train <- sample(1:nrow(train), nrow(train)*0.7) ## split data into train and test
sub_test <- train[-sub_train,]
sub_train <- train[sub_train,]
```

```{r}
plot(density(train$R_moment_2))
plot(density(log(train$R_moment_2)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_2), type = "b")
plot(train$Re, log(train$R_moment_2), type = "b")
```

```{r}
# least square model
r2_ls <- lm(log(R_moment_2) ~ St + Re_factor + Fr_factor, data = sub_train)
r2_ls_int <- lm(log(R_moment_2) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
summary(r2_ls)
summary(r2_ls_int)
```

```{r}
ls_pred <- predict(r2_ls, newdata = sub_test)
mean((sub_test$R_moment_2 - exp(ls_pred))^2)
```

```{r}
ls_pred_int <- predict(r2_ls_int, newdata = sub_test)
mean((sub_test$R_moment_2 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(r2_ls)
```
```{r}
par(mfrow=c(2,2))
plot(r2_ls_int)
```

```{r}
#randomly shuffle data
df.shuffled <- sub_train[sample(nrow(sub_train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_2) ~ poly(St,j) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
r2_poly <- lm(log(R_moment_2) ~ poly(St,7) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
summary(r2_poly)
```
```{r}
poly_pred <- predict(r2_poly, newdata=sub_test)
mean((exp(poly_pred)-sub_test$R_moment_2)^2) 
```


```{r}
#ridge regression
train.matrix <- model.matrix (log(R_moment_2) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, sub_train)[,-1]
test.matrix <- model.matrix (log(R_moment_2) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, sub_test)[,-1]
test.r2 <- sub_test$R_moment_2
train.r2 <- sub_train$R_moment_2

set.seed (1)
cv.out.ridge <- cv.glmnet (train.matrix,train.r2,alpha = 0)
plot(cv.out.ridge)
```

In the plot above, the numbers across the top of the plot are the number of nonzero coefficient estimates for the model. Ridge regression does not set coefficients to 0, so all variables are included in every model. 

```{r}
bestlam.ridge <- cv.out.ridge$lambda.min
r2_ridge <- glmnet(train.matrix,train.r2,alpha=0,lambda=bestlam.ridge)

ridge.pred <- predict(r2_ridge, s = bestlam.ridge, newx = test.matrix)
mean((exp(ridge.pred) - test.r2)^2)
```

```{r}
set.seed(1)
cv.out.lasso <- cv.glmnet (train.matrix,train.r2,alpha =1)
plot(cv.out.lasso)
```

```{r}
bestlam.lasso <- cv.out.lasso$lambda.min
r2.lasso <- glmnet(train.matrix,train.r2,alpha=1,lambda=bestlam.ridge)

lasso.pred <- predict(r2.lasso,s=bestlam.lasso,newx=test.matrix)
mean((exp(lasso.pred)-test.r2)^2)
```



## Non linear modeling

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
set.seed(1)
cv.err <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_2) ~ ns(St, df = i) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
cv.err[i] <- cv.glm(sub_train, model.fit, K=5)$delta[1]
}

plot(4:15, cv.err[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
r2_spline <- glm(log(R_moment_2) ~ ns(St, df = 9) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
attr(bs(sub_train$St, df = 9), "knots")

summary(r2_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r2_spline)
```

```{r}
spline_pred <- predict(r2_spline, newdata = sub_test)
mean((sub_test$R_moment_2 - exp(spline_pred))^2)
```


```{r}
#randomly shuffle data
df.shuffled <- sub_train[sample(nrow(sub_train)),]

#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_2) ~ s(St, j) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_2)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
r2_gam <- gam(log(R_moment_2) ~ s(St, 13) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
```

```{r}
par(mfrow = c(1,3))
plot(r2_gam, se = TRUE, col = "blue")
```

```{r}
gam_pred <- predict(r2_gam, newdata = sub_test)
mean((sub_test$R_moment_2 - exp(gam_pred))^2)
```

```{r}
Model <- c("Least Square Regression", "Polynomial", "Natural Spline", "Generalized Additive Model")
MSE <- c(49849.52, 10378.57, 8775.482, 6290.951)

MSE_table <- data.frame(Model, MSE)
MSE_table
```

## R_moment_4

```{r}
plot(density(train$R_moment_4))
plot(density(log(train$R_moment_4)))

plot(density(train$St))

plot(density(train$Re))

plot(train$St, log(train$R_moment_4), type = "b")
plot(train$Re, log(train$R_moment_4), type = "b")
```

```{r}
# least square model
r4_ls <- lm(log(R_moment_4) ~ St + Re_factor + Fr_factor, data = sub_train)
r4_ls_int <- lm(log(R_moment_4) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
summary(r4_ls)
summary(r4_ls_int)
```

```{r}
ls_pred <- predict(r4_ls, newdata = sub_test)
mean((sub_test$R_moment_4 - exp(ls_pred))^2)
```

```{r}
ls_pred_int <- predict(r4_ls_int, newdata = sub_test)
mean((sub_test$R_moment_4 - exp(ls_pred_int))^2)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls)
```

```{r}
par(mfrow=c(2,2))
plot(r4_ls_int)
```

```{r}
#randomly shuffle data
df.shuffled <- sub_train[sample(nrow(sub_train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(log(R_moment_4) ~ poly(St,j) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
r4_poly <- lm(log(R_moment_4) ~ poly(St,6) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
summary(r4_poly)
```

```{r}
poly_pred <- predict(r4_poly, newdata=sub_test)
mean((exp(poly_pred)-sub_test$R_moment_4)^2) 
```


```{r}
#ridge regression
train.matrix <- model.matrix (log(R_moment_2) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, sub_train)[,-1]
test.matrix <- model.matrix (log(R_moment_2) ~ St + Re_factor + Fr_factor + Re_factor*Fr_factor, sub_test)[,-1]
test.r2 <- sub_test$R_moment_2
train.r2 <- sub_train$R_moment_2

set.seed (1)
cv.out.ridge <- cv.glmnet (train.matrix,train.r2,alpha = 0)
plot(cv.out.ridge)
```

In the plot above, the numbers across the top of the plot are the number of nonzero coefficient estimates for the model. Ridge regression does not set coefficients to 0, so all variables are included in every model. 

```{r}
bestlam.ridge <- cv.out.ridge$lambda.min
r2_ridge <- glmnet(train.matrix,train.r2,alpha=0,lambda=bestlam.ridge)

ridge.pred <- predict(r2_ridge, s = bestlam.ridge, newx = test.matrix)
mean((exp(ridge.pred) - test.r2)^2)
```

```{r}
set.seed(1)
cv.out.lasso <- cv.glmnet (train.matrix,train.r2,alpha =1)
plot(cv.out.lasso)
```

```{r}
bestlam.lasso <- cv.out.lasso$lambda.min
r2.lasso <- glmnet(train.matrix,train.r2,alpha=1,lambda=bestlam.ridge)

lasso.pred <- predict(r2.lasso,s=bestlam.lasso,newx=test.matrix)
mean((exp(lasso.pred)-test.r2)^2)
```

## Non linear modeling

### Natural spline
```{r}
RSS <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_4) ~ ns(St, df = i) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
RSS[i] <- sum(model.fit$residuals^2)
}

plot(4:15, RSS[4:15], type="b", xlab="Degrees of freedom", ylab = "RSS")
title("RSS vs. Degrees of freedom")
```

```{r}
set.seed(1)
cv.err <- rep(0,15)
for (i in 4:15) {
model.fit <- glm(log(R_moment_4) ~ ns(St, df = i) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
cv.err[i] <- cv.glm(sub_train, model.fit, K=5)$delta[1]
}

plot(4:15, cv.err[4:15], type="b", xlab="Degrees of freedom", ylab="Cross validation error")
```


```{r}
r4_spline <- glm(log(R_moment_4) ~ ns(St, df = 9) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
attr(ns(sub_train$St, df = 9), "knots")

summary(r4_spline)
```

```{r}
par(mfrow = c(2,2))
plot(r4_spline)
```

```{r}
spline_pred <- predict(r4_spline, newdata = sub_test)
mean((sub_test$R_moment_4 - exp(spline_pred))^2)
```


```{r}
#randomly shuffle data
df.shuffled <- sub_train[sample(nrow(sub_train)),]

#define number of folds to use for k-fold cross-validation
K <- 10 

#define degree of polynomials to fit
degree <- 15

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = gam(log(R_moment_4) ~ s(St, j) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((exp(fit.test)-testData$R_moment_4)^2) 
    }
}

#find MSE for each degree 
colMeans(mse)
```

```{r}
r4_gam <- gam(log(R_moment_4) ~ s(St, 8) + Re_factor + Fr_factor + Re_factor*Fr_factor, data = sub_train)
```

```{r}
par(mfrow = c(1,3))
plot(r4_gam, se = TRUE, col = "blue")
```

```{r}
gam_pred <- predict(r4_gam, newdata = sub_test)
mean((sub_test$R_moment_4 - exp(gam_pred))^2)
```

```{r}
Model <- c("Least Square Regression", "Polynomial", "Natural Spline", "Generalized Additive Model")
MSE <- c(3.188806e+20, 1.069837e+20, 1.636247e+20, 1.375791e+20)

MSE_table <- data.frame(Model, MSE)
MSE_table
```