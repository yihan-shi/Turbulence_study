---
title: "yihan_shi_casestudy"
author: "Yihan Shi"
date: "10/31/2022"
output: pdf_document
---
# Introduction

"We are experiencing some turbulence, please fasten your seat belt." Many of us might have heard this radio on the plane and felt bumpy. When we mix paint in water, we can also observe turbulence as the color dissipates. Turbulence is so common and easily observed in daily life, yet its causes and effects are hard to predict. In fluid dynamics, turbulence is "characterized by chaotic changes in pressure and flow velocity". With some knowledge and observation in parameters such as fluid density, flow speed, and the property of particles that cluster inside turbulent flows, we can gain insights into the distribution and clustering of particles in idealized turbulence. In this case study, we will investigate 3 observed features that might contribute to particle distribution in turbulence: Reynolds number (Re), which takes flow speed, viscosity, and density into account; Gravitational acceleration (Fr); Stokes number (St) that quantifies particle characteristics like size, relaxation time, and particle diameter. We hope to use these 3 features to explain changes in particle distribution as well as extrapolate beyond the scope of the known observations.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(plyr)
library(gam)
library(boot)
library(splines)
library(kableExtra)
library(glmnet)
```

```{r}
# load data
set.seed(1)
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv") 

train$Fr <- 25/(1+exp(2*(0.3 - train$Fr))) # cite, justify the choice 
test$Fr <- 25/(1+exp(2*(0.3 - test$Fr))) 

sub_train <- sample(1:nrow(train), nrow(train)*0.7)
sub_test <- train[-sub_train,]
sub_train <- train[sub_train,]
```

```{r echo = FALSE}
# eda
# plot(dist(sub_train$R_moment_1))
pairs(sub_train)
```
From the pair plot, there is somewhat linear or polynomial relationship between St and R_moment_1. 

### Polynomial regression
```{r}
#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(train)),breaks=K,labels=FALSE)

# metrics
mse <- matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- train[testIndexes, ]
    trainData <- train[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train <- lm(R_moment_1 ~ poly(St,j) + Re + Fr + Re * Fr,
                         data = trainData)
        fit.test <- predict(fit.train, newdata = testData)

        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
        mse[i,j] <- mean((exp(fit.test)-testData$R_moment_1)^2)

    }
}

x <- 1:10
mean_mse_linear <- colMeans(mse)
min_mse_linear <- min(mse)
mean_r2_linear <- colMeans(adj.r2)
max_r2_linear <- max(adj.r2)


plot(x, mean_mse_linear, type = "l", xlab = "Polynomial Degree", ylab = "MSE", main = "Polynomial regression")
plot(c(1:10), mean_r2_linear, xlab = "Polynomial Degree", ylab = "Adjusted R^2", main = "Polynomial regression", type = "l")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ decreases as polynomial degree increases, and the lowest MSE is achieved at degree of 4. We found that a linear regression is not suitable. Even though it has a relatively high adjusted $R^2$, the MSE is way higher than other polynomial models. We decided that the optimal polynomial order for St is 4. After degree of 4, the adjusted $R^2$ starts to decrease quickely. 

We tried removing data of both high leverage and residual. However, this didn't change the MSE and adjusted $R^2$ greatly. Since these observations take up 7% of the full training data, we decided that we don't want to exclude them.

### Natural spline
```{r}
#define degree of freedoms to fit
df <- 13

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=df)
adj.r2 <- matrix(data=NA,nrow=K,ncol=df)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- train[testIndexes, ]
    trainData <- train[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:df){
        fit.train <- glm(R_moment_1 ~ ns(St, df = j) + Fr + Re + Fr * Re,
           data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((exp(fit.test)-testData$R_moment_1)^2) 
    }
}

#find MSE for each degree 
mean_mse_ns <- colMeans(mse)
min_mse_ns <- min(mse)
mean_r2_ns <- colMeans(adj.r2)
max_r2_ns <- max(adj.r2)
plot(c(1:df), mean_mse_ns, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "Natural spline")
plot(c(1:df), mean_r2_ns, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "Natural spline")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases, and the lowest MSE is achieved at degree of 13. The optimal polynomial degrees of freedom for St is 13, which achieves both the highest adjusted $R^2$ and MSE.

### GAM
```{r}
#define degree of freedoms to fit
degree <- 20

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- train[testIndexes, ]
    trainData <- train[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
      # print(paste0(i,j))
      fit.train <- gam::gam(R_moment_1 ~ s(St, df = j) + Fr + Re + Fr * Re,
         data = trainData)
      adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
      fit.test <- predict(fit.train, newdata=testData)
      mse[i,j] <- mean((exp(fit.test)-testData$R_moment_1)^2) 
    }
}

#find MSE for each degree 
mean_mse_gam <- colMeans(mse)
min_mse_gam <- min(mse)
mean_r2_gam <- colMeans(adj.r2)
max_r2_gam <- max(adj.r2)
plot(c(1:degree), mean_mse_gam, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "GAM")
plot(c(1:degree), mean_r2_gam, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "GAM")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases. The optimal degree of freedom for GAM model to achieve the lowest MSE is 5. However, we might conculde that a degree of freedom around 14 is optimal because it doesn't have a big increase in MSE compared to degree of freedom at 5. It also achieves the highest adjusted $R^2$.


```{r}
# make a MSE table
models <- c("Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(min_mse_linear, 
         min_mse_ns, 
         min_mse_gam)
adj.R <- c(max_r2_linear,
           max_r2_ns,
           max_r2_gam)
formula <- c("log(R_moment_1) ~ Fr + Re + poly(St, 4) + Fr * Re",
             "log(R_moment_1) ~ ns(St, df = 13) + Fr + Re + Fr * Re",
             "log(R_moment_1) ~ s(St, 14) + Re + Fr + Fr * Re"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kbl() %>% 
  kable_styling()
```

For prediction: GAM is the best because it has the highest adjusted $R^2$. However, since we cannot intrepret the coefficients of each predictor, we would use polynomial regression for inference.
