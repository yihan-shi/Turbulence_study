---
title: "yihan_shi_casestudy"
author: "Yihan Shi"
date: "10/31/2022"
output: pdf_document
---
# Introduction

"We are experiencing some turbulence, please fasten your seat belt." Many of us might have heard this radio on the plane and felt bumpy. When we mix paint in water, we can also observe turbulence as the color dissipates. Turbulence is so common and easily observed in daily life, yet its causes and effects are hard to predict. In fluid dynamics, turbulence is "characterized by chaotic changes in pressure and flow velocity". With some knowledge and observation in parameters such as fluid density, flow speed, and the property of particles that cluster inside turbulent flows, we can gain insights into the distribution and clustering of particles in idealized turbulence. In this case study, we will investigate 3 observed features that might contribute to particle distribution in turbulence: Reynolds number (Re), which takes flow speed, viscosity, and density into account; Gravitational acceleration (Fr); Stokes number (St) that quantifies particle characteristics like size, relaxation time, and particle diameter. We hope to use these 3 features to explain changes in particle distribution as well as extrapolate beyond the scope of the known observations.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(plyr)
library(gam)
library(boot)
library(splines)
library(kableExtra)
library(glmnet)
```

```{r}
# load data
set.seed(1)
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv") 

train$Fr <- 25/(1+exp(2*(0.3 - train$Fr))) # cite, justify the choice
test$Fr <- 25/(1+exp(2*(0.3 - test$Fr)))

```

```{r echo = FALSE}
# eda
# plot(dist(sub_train$R_moment_1))
pairs(train)
```

From the pair plot, there is somewhat linear or polynomial relationship between St and R_moment_1. 

### Linear regression
```{r}
df.shuffled <- train[sample(nrow(train)),]
K <- 5

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)), breaks=K, labels=FALSE)

#create object to hold MSE's of models
mse <- rep(NA, K)
adj.r2 <- rep(NA, K)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    fit.train <-  lm(log(R_moment_1) ~ St + Re + as.factor(Fr) + Re*as.factor(Fr), data = trainData)
    fit.test <- predict(fit.train, newdata=testData)
    mse[i] <- mean((exp(fit.test)-testData$R_moment_1)^2) 
    adj.r2[i] <- summary(fit.train)$adj.r.squared
}

#find MSE
mse_linear <- mean(mse)
mse_adjr2 <- mean(adj.r2)
```

### Polynomial regression
```{r}
#randomly shuffle data
df.shuffled <- train[sample(nrow(train)),]

#define number of folds to use for k-fold cross-validation
K <- 5

#define degree of polynomials to fit
degree <- 10

#create k equal-sized folds
folds <- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train <- lm(R_moment_1 ~ poly(St,j) + Re+ Fr + Re*Fr, 
                       data = trainData)
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((fit.test-testData$R_moment_1)^2) 
        adj.r2[i,j] <- summary(fit.train)$adj.r.squared
    }
}

x <- 1:10
mean_mse_linear <- colMeans(mse)
min_mse_linear <- min(mse)
mean_r2_linear <- colMeans(adj.r2)
max_r2_linear <- max(adj.r2)


plot(x, mean_mse_linear, type = "l", xlab = "Polynomial Degree", ylab = "MSE", main = "Polynomial regression")
plot(c(1:10), mean_r2_linear, xlab = "Polynomial Degree", ylab = "Adjusted R^2", main = "Polynomial regression", type = "l")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ decreases as polynomial degree increases, while MSE increases as polynomial degree increases. The lowest MSE is achieved at degree of 4. We found that a linear regression is not suitable. Even though it has a relatively high adjusted $R^2$, the MSE is way higher than other polynomial models. We decided that the optimal polynomial order for St is 4. After degree of 4, the adjusted $R^2$ starts to decrease quickely. 

We tried removing data of both high leverage and residual. However, this didn't change the MSE and adjusted $R^2$ greatly. Since these observations take up 7% of the full training data, we decided that we don't want to exclude them.

### Natural spline
```{r}
df.shuffled <- train[sample(nrow(train)),]
df <- 13

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=df)
adj.r2 <- matrix(data=NA,nrow=K,ncol=df)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:df){
        fit.train <- glm(R_moment_1 ~ ns(St, df = j) + Fr + Re + Fr * Re,
           data = trainData)
        adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
        # adj.r2[i,j] <- summary(fit.train)$adj.r.squared
        fit.test <- predict(fit.train, newdata=testData)
        mse[i,j] <- mean((fit.test-testData$R_moment_1)^2) 
    }
}

#find MSE for each degree 
mean_mse_ns <- colMeans(mse)
min_mse_ns <- min(mse)
mean_r2_ns <- colMeans(adj.r2)
max_r2_ns <- max(adj.r2)
plot(c(1:df), mean_mse_ns, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "Natural spline")
plot(c(1:df), mean_r2_ns, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "Natural spline")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases, and the lowest MSE is achieved at degree of 13. The optimal polynomial degrees of freedom for St is 8, which achieves the lowest MSE and relatively high adjusted $R^2$.

### GAM
```{r}
df.shuffled <- train[sample(nrow(train)),]
degree <- 20

#create object to hold MSE's of models
mse <- matrix(data=NA,nrow=K,ncol=degree)
adj.r2 <- matrix(data=NA,nrow=K,ncol=degree)

#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- df.shuffled[testIndexes, ]
    trainData <- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
      # print(paste0(i,j))
      fit.train <- gam::gam(R_moment_1 ~ s(St, df = j) + Fr + Re + Fr * Re,
         data = trainData)
      adj.r2[i,j] <- with(summary(fit.train), 1 - deviance/null.deviance)
      fit.test <- predict(fit.train, newdata=testData)
      mse[i,j] <- mean((fit.test-testData$R_moment_1)^2) 
    }
}

#find MSE for each degree 
mean_mse_gam <- colMeans(mse)
min_mse_gam <- min(mse)
mean_r2_gam <- colMeans(adj.r2)
max_r2_gam <- max(adj.r2)
plot(c(1:degree), mean_mse_gam, xlab = "Degree of Freedom", ylab = "MSE", 
     type = "l", main = "GAM")
plot(c(1:degree), mean_r2_gam, xlab = "Degree of Freedom", ylab = "Adjusted R^2",
     type = "l", main = "GAM")
```
Using cross-validation with 5 folds, we see the adjusted $R^2$ increases as the degree of freedom increases. The optimal degree of freedom for GAM model to achieve the lowest MSE is 5. However, we might conculde that a degree of freedom around 14 is optimal because it doesn't have a big increase in MSE compared to degree of freedom at 5. It also achieves the highest adjusted $R^2$.


```{r}
# make a MSE table
models <- c("Linear regresion", 
            "Polynomial regression", 
            "Natural spline", 
            "Generalized additive model")
mse <- c(mse_linear,
         min_mse_linear, 
         min_mse_ns, 
         min_mse_gam)
adj.R <- c(mse_adjr2,
           max_r2_linear,
           max_r2_ns,
           max_r2_gam)
formula <- c("R_moment_1 ~ Fr + Re + St + Fr * Re",
             "R_moment_1 ~ Fr + Re + poly(St, 4) + Fr * Re",
             "R_moment_1 ~ ns(St, df = 8) + Fr + Re + Fr * Re",
             "R_moment_1 ~ s(St, 14) + Re + Fr + Fr * Re"
             )
df <- data.frame(models, formula, mse, adj.R)
df %>% 
  kable(caption = "Model MSE and Adjusted $R^2$") %>% 
  kable_styling() %>% 
  add_footnote(c("Fr logit-transformed"), notation = "symbol")
```

For prediction: GAM is the best because it has the highest adjusted $R^2$. However, since we cannot intrepret the coefficients of each predictor, we would use polynomial regression for inference.
