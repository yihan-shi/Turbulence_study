---
title: "yihan_shi_casestudy"
author: "Yihan Shi"
date: "10/31/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(plyr)
library(gam)
library(boot)
library(splines)
```

```{r}
# load data
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
set.seed(1)
```

```{r}
# eda
plot(dist(train$R_moment_1))
pairs(train)
```
From the pair plot, there is somewhat linear relationship between St and R_moment_1. 


## predictive modeling
### linear regression
```{r}
# start from linear model of St
# fit model
lm1 <- lm(R_moment_1 ~ poly(St,1), data = train)
summary(lm1)
(mse <- mean(lm1$residuals^2))

plot(R_moment_1 ~ St,
     data = train,
     col = "darkgrey")
lines(seq(from = range(train$St)[1], to = range(train$St)[2], by = 0.1), 
      predict(lm1, list(St = seq(from = range(train$St)[1], 
                                 to = range(train$St)[2], by = 0.1))),
      col = "red",
      type = "l")
```
Linear regression or polynomial regression with St alone is not suitable.

```{r}
# start from linear model of St
# lm2 <- lm(R_moment_1 ~ Re, data = train)
lm2 <- lm(R_moment_1 ~ as.factor(Re), data = train)
summary(lm2)
(mse <- mean(lm2$residuals^2))
plot(R_moment_1 ~ as.factor(Re),
     data = train)
```
The relationship between Re and R_moment_1 is not linear. R^2 is only 0.6 and MSE = 0.0012. Setting Re to be categorical improves the model accuracy. R^2 is 0.889 and MSE = 0.0003 

```{r}
lm3 <- lm(R_moment_1 ~ as.factor(Fr), data = train)
summary(lm3)
(mse <- mean(lm3$residuals^2))
plot(R_moment_1 ~ as.factor(Fr),
     data = train)
```
```{r}
# find out the optimal polynomial degrees for St
degrees <- rep(NA, 10)

for (i in 1:10) {
fit <- glm(R_moment_1 ~ as.factor(Fr) + as.factor(Re) + poly(St, i), data = train)
degrees[i] <- cv.glm(train, fit, K = 5)$delta[1]
}

plot(degrees, xlab = "Polynomial", ylab = "MSE", type = "l")

lm4 <- lm(R_moment_1 ~ as.factor(Fr) + as.factor(Re) + poly(St, 2), data = train)
summary(lm4)
(mse <- mean(lm4$residuals^2))
plot(lm4)
```
This linear model gives us a decent R^2 (0.9243) and MSE (0.0002). However, 
the residuals are not randomly scattered, so linear regression might not be 
suitable.

```{r}
# remove high leverage point and refit
HighLeverage <- cooks.distance(lm4) > (4/nrow(train))
LargeResiduals <- rstudent(lm4) > 3
train_2 <- train[!HighLeverage & !LargeResiduals,]
lm5 <-lm(R_moment_1 ~ as.factor(Fr) + as.factor(Re) + poly(St, 2), data = train_2)
summary(lm5)
(mse <- mean(lm5$residuals^2))
plot(lm5)
```
After removing data of both high leverage and residual, R^2 (0.95) slightly improved, and MSE decreases to 0.00014.

However, we might not want to exclude these data since they take up 7.8% of the full training data.

### natural spline
```{r}
ns_1 <- lm(R_moment_1 ~ ns(St, df = 4) + as.factor(Fr) + as.factor(Re),
           data = train)
summary(ns_1)
(mse <- mean(ns_1$residuals^2))
# St.grid <- seq(from = range(train$St)[1], to = range(train$St)[2])
# pred_1 <- predict(ns_1, newdata = list(St = St.grid, 
#                                        Fr = as.factor(train$Fr),
#                                        Re = as.factor(train$Re)), se = TRUE)
# plot(train$St, train$R_moment_1, col = "gray")
# lines(St.grid, pred_1$fit, col = "red", lwd = 2)
```
### smooth spline (Weird)
```{r}
sm_sp <- smooth.spline(train$St, 
                       train$R_moment_1, 
                       train$Re,
                       cv = TRUE)
sm_sp$df
(mse <- mean(sm_sp$residuals^2))
plot(train$St, train$R_moment_1, col = "gray")
lines(sm_sp, col = "blue", lwd = 2)
```
## Prediction

### GAM

```{r}
gam_train <- train
gam_train$Fr <- as.factor(train$Fr)
gam_train$Re <- as.factor(train$Re)
g <- gam(R_moment_1 ~ s(St, 4) + Re + Fr,
           data = gam_train)
par(mfrow = c(1,3))
plot(g, se = TRUE, col = "blue", all.terms = TRUE)
summary(gam)
```

```{r}
pred_gam <- predict(gam)
(mean((train$R_moment_1 - pred_gam)^2)) 
```

```{r}
train$C_moment_1 <- 0
train$C_moment_2 <- train$R_moment_2 - (train$R_moment_1)^2
train$C_moment_3 <- train$R_moment_3 - 3*train$R_moment_1*train$R_moment_2 + 2*(train$R_moment_1)^3
train$C_moment_4 <- train$R_moment_4 - 4*train$R_moment_1*train$R_moment_3 + 6*(train$R_moment_1)^2*train$R_moment_2 -3*(train$R_moment_1)^4
```

